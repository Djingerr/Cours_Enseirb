# Introduction à la théorie de l’information

## Introduction

### Contexte général
La **théorie de l’information** est une branche des mathématiques fondée en **1948** par **Claude Elwood Shannon (1916–2001)**.  
Elle naît dans le contexte des **télécommunications**, avec pour objectif fondamental de comprendre, mesurer et optimiser la **transmission de l’information**.

L’article fondateur est :
> *A Mathematical Theory of Communication*, Bell System Technical Journal, 1948.

Cette théorie a profondément influencé :
- les télécommunications,
- l’informatique (compression, cryptographie),
- la physique statistique,
- la biologie des systèmes,
- la finance quantitative.

### Problématique centrale
> **Comment transmettre une information d’un point à un autre de manière fiable et efficace, malgré le bruit ?**

### Objectifs d’apprentissage
À l’issue de ce cours, l’étudiant doit être capable de :
- comprendre et manipuler les notions d’**entropie** et d’**information mutuelle**,
- maîtriser les fondements du **codage source** (compression),
- comprendre les principes du **codage canal** (correction d’erreurs),
- appliquer des algorithmes concrets comme le **codage de Huffman**.

### Pré-requis
- Bases de probabilités
- Bases de programmation
- Notions élémentaires de Python (NumPy / SciPy)

---

## Liens
- ⏭️ Chapitre suivant : [[01_Paradigme_de_Shannon]]
